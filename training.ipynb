{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data for model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found GPU: /device:GPU:0\n",
      "(2494, 60)\n",
      "(624, 60)\n",
      "input shapes, training:(2494, 60, 1)\n",
      "output shapes, training:(2494,)\n"
     ]
    }
   ],
   "source": [
    "from tcn import TCN, tcn_full_summary\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_max_light(curve):\n",
    "    id_max = curve[curve[\"cts\"] != 0]['cts'].idxmax()\n",
    "    max_data = curve.loc[id_max, :]\n",
    "    return id_max, max_data\n",
    "\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "def plot_model_history(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "data_dir = \"./TESS_data/processed_curves/\"\n",
    "files = os.listdir(data_dir)\n",
    "batch_size = 100\n",
    "timesteps = 60\n",
    "n_features = 1\n",
    "\n",
    "# #format data properly\n",
    "test_size = int(0.8 * len(files))\n",
    "\n",
    "def min_max_norm(cts):\n",
    "    return cts - cts.min() / (cts.max() - cts.min())\n",
    "\n",
    "def format_data(data, include_uncertainty=False):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for csv in data:\n",
    "        df_x = pd.read_csv(data_dir + csv, index_col=\"relative_time\")\n",
    "        df_x.index = df_x.index + 1.0\n",
    "        max_time, max_light = find_max_light(df_x)\n",
    "        for t in range(df_x.index.size, timesteps+1):\n",
    "            if not t in df_x.index:\n",
    "                df_x.loc[t] = [0.0, 0.0]\n",
    "\n",
    "        if include_uncertainty:\n",
    "            df_y = np.array([max_time, max_light['e_cts']])\n",
    "        else:\n",
    "            df_y = max_time\n",
    "            df_x = df_x['cts']\n",
    "            if df_x.shape[0] > 60:\n",
    "                print(csv)\n",
    "\n",
    "        x.append(df_x)\n",
    "        y.append(df_y)\n",
    "\n",
    "    #reshapes data to be (num_samples, timesteps, num_features), instead of (num_samples, timesteps)\n",
    "    def add_feature_dim(input):\n",
    "        print(input.shape)\n",
    "        return input.reshape(input.shape[0], input.shape[1], n_features)\n",
    "\n",
    "    x_train = add_feature_dim(np.array(x[:test_size]))\n",
    "    x_test = add_feature_dim(np.array(x[test_size:]))\n",
    "    y_train = np.array(y[:test_size])\n",
    "    y_test = np.array(y[test_size:])\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = format_data(files)\n",
    "# [num_samples, timesteps, num_features)\n",
    "print(f\"input shapes, training:{x_train.shape}\")\n",
    "# [num samples,]\n",
    "print(f\"output shapes, training:{y_train.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.0\n"
     ]
    }
   ],
   "source": [
    "print(y_train.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple LSTM\n",
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "78/78 [==============================] - 62s 770ms/step - loss: 5201422336.0000 - val_loss: 186222256.0000\n",
      "Epoch 2/200\n",
      "78/78 [==============================] - 29s 369ms/step - loss: 5398770688.0000 - val_loss: 46938906624.0000\n",
      "Epoch 3/200\n",
      "78/78 [==============================] - 28s 353ms/step - loss: 34370633728.0000 - val_loss: 434300032.0000\n",
      "Epoch 4/200\n",
      "78/78 [==============================] - 26s 332ms/step - loss: 16739159040.0000 - val_loss: 310691488.0000\n",
      "Epoch 5/200\n",
      "78/78 [==============================] - 25s 323ms/step - loss: 11060960256.0000 - val_loss: 51615196.0000\n",
      "Epoch 6/200\n",
      "78/78 [==============================] - 22s 278ms/step - loss: 389103040.0000 - val_loss: 41559380.0000\n",
      "Epoch 7/200\n",
      "78/78 [==============================] - 22s 288ms/step - loss: 173698465792.0000 - val_loss: 55529668.0000\n",
      "Epoch 8/200\n",
      "78/78 [==============================] - 23s 289ms/step - loss: 1604560256.0000 - val_loss: 20462048.0000\n",
      "Epoch 9/200\n",
      "78/78 [==============================] - 28s 363ms/step - loss: 661923968.0000 - val_loss: 13971466.0000\n",
      "Epoch 10/200\n",
      "78/78 [==============================] - 26s 333ms/step - loss: 160612272.0000 - val_loss: 26161206.0000\n",
      "Epoch 11/200\n",
      "78/78 [==============================] - 22s 287ms/step - loss: 1481472384.0000 - val_loss: 312880000.0000\n",
      "Epoch 12/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 624231744.0000 - val_loss: 3209082112.0000\n",
      "Epoch 13/200\n",
      "78/78 [==============================] - 22s 289ms/step - loss: 245156003840.0000 - val_loss: 56383811584.0000\n",
      "Epoch 14/200\n",
      "78/78 [==============================] - 22s 289ms/step - loss: 1702945488896.0000 - val_loss: 62587384.0000\n",
      "Epoch 15/200\n",
      "78/78 [==============================] - 22s 289ms/step - loss: 4392549376.0000 - val_loss: 21855172.0000\n",
      "Epoch 16/200\n",
      "78/78 [==============================] - 23s 298ms/step - loss: 1860508928.0000 - val_loss: 39470556.0000\n",
      "Epoch 17/200\n",
      "78/78 [==============================] - 23s 297ms/step - loss: 3173298432.0000 - val_loss: 16977874.0000\n",
      "Epoch 18/200\n",
      "78/78 [==============================] - 21s 275ms/step - loss: 2187808768.0000 - val_loss: 32277360.0000\n",
      "Epoch 19/200\n",
      "78/78 [==============================] - 25s 325ms/step - loss: 1583441152.0000 - val_loss: 202843280.0000\n",
      "Epoch 20/200\n",
      "78/78 [==============================] - 27s 347ms/step - loss: 715049280.0000 - val_loss: 190486192.0000\n",
      "Epoch 21/200\n",
      "78/78 [==============================] - 23s 291ms/step - loss: 616525248.0000 - val_loss: 135720960.0000\n",
      "Epoch 22/200\n",
      "78/78 [==============================] - 24s 311ms/step - loss: 433172512.0000 - val_loss: 8453130.0000\n",
      "Epoch 23/200\n",
      "78/78 [==============================] - 23s 290ms/step - loss: 311055072.0000 - val_loss: 7681853.5000\n",
      "Epoch 24/200\n",
      "78/78 [==============================] - 22s 286ms/step - loss: 262150176.0000 - val_loss: 5561979.5000\n",
      "Epoch 25/200\n",
      "78/78 [==============================] - 21s 269ms/step - loss: 179721168.0000 - val_loss: 4983744.5000\n",
      "Epoch 26/200\n",
      "78/78 [==============================] - 22s 283ms/step - loss: 144520592.0000 - val_loss: 5756047.5000\n",
      "Epoch 27/200\n",
      "78/78 [==============================] - 22s 284ms/step - loss: 107450880.0000 - val_loss: 258268112.0000\n",
      "Epoch 28/200\n",
      "78/78 [==============================] - 30s 380ms/step - loss: 41270784.0000 - val_loss: 282625120.0000\n",
      "Epoch 29/200\n",
      "78/78 [==============================] - 33s 420ms/step - loss: 25116760.0000 - val_loss: 7088292.0000\n",
      "Epoch 30/200\n",
      "78/78 [==============================] - 34s 442ms/step - loss: 20060096.0000 - val_loss: 6833490.0000\n",
      "Epoch 31/200\n",
      "78/78 [==============================] - 23s 289ms/step - loss: 10624083.0000 - val_loss: 5957058.5000\n",
      "Epoch 32/200\n",
      "78/78 [==============================] - 23s 300ms/step - loss: 11367801.0000 - val_loss: 2053259.7500\n",
      "Epoch 33/200\n",
      "78/78 [==============================] - 22s 284ms/step - loss: 26303286.0000 - val_loss: 1785760.3750\n",
      "Epoch 34/200\n",
      "78/78 [==============================] - 23s 294ms/step - loss: 14684472.0000 - val_loss: 3469221.7500\n",
      "Epoch 35/200\n",
      "78/78 [==============================] - 27s 343ms/step - loss: 13505226.0000 - val_loss: 2277943.7500\n",
      "Epoch 36/200\n",
      "78/78 [==============================] - 53s 687ms/step - loss: 25871128.0000 - val_loss: 2204472.5000\n",
      "Epoch 37/200\n",
      "78/78 [==============================] - 98s 1s/step - loss: 8930906.0000 - val_loss: 2310455.0000\n",
      "Epoch 38/200\n",
      "78/78 [==============================] - 204s 3s/step - loss: 54729400.0000 - val_loss: 4726217.5000\n",
      "Epoch 39/200\n",
      "78/78 [==============================] - 99s 1s/step - loss: 52979344.0000 - val_loss: 4612064.0000\n",
      "Epoch 40/200\n",
      "78/78 [==============================] - 221s 3s/step - loss: 40988044.0000 - val_loss: 6617398.5000\n",
      "Epoch 41/200\n",
      "78/78 [==============================] - 27s 343ms/step - loss: 205184640.0000 - val_loss: 4931475.5000\n",
      "Epoch 42/200\n",
      "78/78 [==============================] - 25s 315ms/step - loss: 88002840.0000 - val_loss: 9807236.0000\n",
      "Epoch 43/200\n",
      "78/78 [==============================] - 24s 313ms/step - loss: 18538958.0000 - val_loss: 6834650.0000\n",
      "Epoch 44/200\n",
      "78/78 [==============================] - 22s 283ms/step - loss: 19292776.0000 - val_loss: 6126749.0000\n",
      "Epoch 45/200\n",
      "78/78 [==============================] - 22s 286ms/step - loss: 68480000.0000 - val_loss: 10118059.0000\n",
      "Epoch 46/200\n",
      "78/78 [==============================] - 21s 274ms/step - loss: 29117274.0000 - val_loss: 10814788.0000\n",
      "Epoch 47/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 492169408.0000 - val_loss: 48857824.0000\n",
      "Epoch 48/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 412395872.0000 - val_loss: 11211934.0000\n",
      "Epoch 49/200\n",
      "78/78 [==============================] - 20s 262ms/step - loss: 53166532.0000 - val_loss: 10717422.0000\n",
      "Epoch 50/200\n",
      "78/78 [==============================] - 21s 264ms/step - loss: 25179218.0000 - val_loss: 9682464.0000\n",
      "Epoch 51/200\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 65982380.0000 - val_loss: 9590297.0000\n",
      "Epoch 52/200\n",
      "78/78 [==============================] - 20s 263ms/step - loss: 59871296.0000 - val_loss: 4092977.2500\n",
      "Epoch 53/200\n",
      "78/78 [==============================] - 21s 265ms/step - loss: 38467864.0000 - val_loss: 4206176.5000\n",
      "Epoch 54/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 27098486.0000 - val_loss: 3947011.2500\n",
      "Epoch 55/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 167006960.0000 - val_loss: 4437120.0000\n",
      "Epoch 56/200\n",
      "78/78 [==============================] - 21s 274ms/step - loss: 106123528.0000 - val_loss: 4315912.0000\n",
      "Epoch 57/200\n",
      "78/78 [==============================] - 21s 265ms/step - loss: 211778368.0000 - val_loss: 6201393.0000\n",
      "Epoch 58/200\n",
      "78/78 [==============================] - 21s 270ms/step - loss: 485889280.0000 - val_loss: 5066554.5000\n",
      "Epoch 59/200\n",
      "78/78 [==============================] - 23s 291ms/step - loss: 219932528.0000 - val_loss: 3431116.5000\n",
      "Epoch 60/200\n",
      "78/78 [==============================] - 22s 284ms/step - loss: 179312208.0000 - val_loss: 3349985.2500\n",
      "Epoch 61/200\n",
      "78/78 [==============================] - 21s 272ms/step - loss: 108047392.0000 - val_loss: 3497022.2500\n",
      "Epoch 62/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 92675584.0000 - val_loss: 3615832.2500\n",
      "Epoch 63/200\n",
      "78/78 [==============================] - 20s 261ms/step - loss: 134216520.0000 - val_loss: 964603.6875\n",
      "Epoch 64/200\n",
      "78/78 [==============================] - 23s 289ms/step - loss: 170954496.0000 - val_loss: 2718440.5000\n",
      "Epoch 65/200\n",
      "78/78 [==============================] - 21s 269ms/step - loss: 106301360.0000 - val_loss: 2835542.2500\n",
      "Epoch 66/200\n",
      "78/78 [==============================] - 20s 260ms/step - loss: 69879976.0000 - val_loss: 2674176.2500\n",
      "Epoch 67/200\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 94559656.0000 - val_loss: 2535952.7500\n",
      "Epoch 68/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 54092972.0000 - val_loss: 7829131.5000\n",
      "Epoch 69/200\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 41542040.0000 - val_loss: 7983117.0000\n",
      "Epoch 70/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 36972836.0000 - val_loss: 7990564.0000\n",
      "Epoch 71/200\n",
      "78/78 [==============================] - 21s 274ms/step - loss: 5259237.5000 - val_loss: 8278016.0000\n",
      "Epoch 72/200\n",
      "78/78 [==============================] - 21s 274ms/step - loss: 23361262.0000 - val_loss: 2788847.0000\n",
      "Epoch 73/200\n",
      "78/78 [==============================] - 21s 271ms/step - loss: 35099712.0000 - val_loss: 9609086.0000\n",
      "Epoch 74/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 27831676.0000 - val_loss: 2212546.7500\n",
      "Epoch 75/200\n",
      "78/78 [==============================] - 21s 272ms/step - loss: 5464273.5000 - val_loss: 2701164.0000\n",
      "Epoch 76/200\n",
      "78/78 [==============================] - 21s 265ms/step - loss: 5094782.0000 - val_loss: 2808821.2500\n",
      "Epoch 77/200\n",
      "78/78 [==============================] - 20s 262ms/step - loss: 5682556.0000 - val_loss: 2409773.2500\n",
      "Epoch 78/200\n",
      "78/78 [==============================] - 21s 266ms/step - loss: 13283923.0000 - val_loss: 3367451.7500\n",
      "Epoch 79/200\n",
      "78/78 [==============================] - 22s 276ms/step - loss: 6277058.0000 - val_loss: 953557.4375\n",
      "Epoch 80/200\n",
      "78/78 [==============================] - 21s 274ms/step - loss: 12676083.0000 - val_loss: 1618866.6250\n",
      "Epoch 81/200\n",
      "78/78 [==============================] - 21s 275ms/step - loss: 11713125.0000 - val_loss: 2277661.2500\n",
      "Epoch 82/200\n",
      "78/78 [==============================] - 21s 274ms/step - loss: 10589861.0000 - val_loss: 1091430.6250\n",
      "Epoch 83/200\n",
      "78/78 [==============================] - 26s 339ms/step - loss: 8187923.5000 - val_loss: 1339220.0000\n",
      "Epoch 84/200\n",
      "78/78 [==============================] - 21s 276ms/step - loss: 9120431.0000 - val_loss: 768341.3125\n",
      "Epoch 85/200\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 5460595.0000 - val_loss: 916169.6250\n",
      "Epoch 86/200\n",
      "78/78 [==============================] - 20s 261ms/step - loss: 4050813.0000 - val_loss: 836907.0000\n",
      "Epoch 87/200\n",
      "78/78 [==============================] - 21s 266ms/step - loss: 3073819.0000 - val_loss: 501316.0000\n",
      "Epoch 88/200\n",
      "78/78 [==============================] - 22s 284ms/step - loss: 3064843.5000 - val_loss: 2358327.2500\n",
      "Epoch 89/200\n",
      "78/78 [==============================] - 21s 274ms/step - loss: 26133610.0000 - val_loss: 1688733.6250\n",
      "Epoch 90/200\n",
      "78/78 [==============================] - 21s 264ms/step - loss: 85085136.0000 - val_loss: 1865565.1250\n",
      "Epoch 91/200\n",
      "78/78 [==============================] - 20s 261ms/step - loss: 58614700.0000 - val_loss: 1849579.5000\n",
      "Epoch 92/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 14504591.0000 - val_loss: 1344539.1250\n",
      "Epoch 93/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 11519751.0000 - val_loss: 1320107.1250\n",
      "Epoch 94/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 4027996.5000 - val_loss: 1191710.6250\n",
      "Epoch 95/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 2212478.7500 - val_loss: 1247028.1250\n",
      "Epoch 96/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 5541196.0000 - val_loss: 1249079.1250\n",
      "Epoch 97/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 12422643.0000 - val_loss: 1378568.8750\n",
      "Epoch 98/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 9966342.0000 - val_loss: 2037782.3750\n",
      "Epoch 99/200\n",
      "78/78 [==============================] - 20s 260ms/step - loss: 10223994.0000 - val_loss: 1187044.3750\n",
      "Epoch 100/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 3950564.0000 - val_loss: 3130131.2500\n",
      "Epoch 101/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 6294240.0000 - val_loss: 1571959.7500\n",
      "Epoch 102/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 24497184.0000 - val_loss: 1276822.2500\n",
      "Epoch 103/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 17190620.0000 - val_loss: 2546347.5000\n",
      "Epoch 104/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 10314831.0000 - val_loss: 1293522.7500\n",
      "Epoch 105/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 5732057.5000 - val_loss: 1632611.8750\n",
      "Epoch 106/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 28605180.0000 - val_loss: 293004.5312\n",
      "Epoch 107/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 4094932.5000 - val_loss: 792069.6875\n",
      "Epoch 108/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 19150978.0000 - val_loss: 653629.8125\n",
      "Epoch 109/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 11573341.0000 - val_loss: 1935376.2500\n",
      "Epoch 110/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 7860379.0000 - val_loss: 480979.6875\n",
      "Epoch 111/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 5526220.5000 - val_loss: 798981.2500\n",
      "Epoch 112/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 3677973.5000 - val_loss: 378530.5000\n",
      "Epoch 113/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 8717002.0000 - val_loss: 350037.5625\n",
      "Epoch 114/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 10800778.0000 - val_loss: 499044.8750\n",
      "Epoch 115/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 18242422.0000 - val_loss: 1239344.7500\n",
      "Epoch 116/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 34683284.0000 - val_loss: 2172045.2500\n",
      "Epoch 117/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 56671328.0000 - val_loss: 951434.2500\n",
      "Epoch 118/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 44866964.0000 - val_loss: 632999.5625\n",
      "Epoch 119/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 29218726.0000 - val_loss: 320609.9688\n",
      "Epoch 120/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 11574544.0000 - val_loss: 213716.9375\n",
      "Epoch 121/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 7450306.0000 - val_loss: 1011901.8750\n",
      "Epoch 122/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 14310465.0000 - val_loss: 1553572.6250\n",
      "Epoch 123/200\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 11638510.0000 - val_loss: 1303342.2500\n",
      "Epoch 124/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 11058034.0000 - val_loss: 844389.2500\n",
      "Epoch 125/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 10964769.0000 - val_loss: 642905.5625\n",
      "Epoch 126/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 11488257.0000 - val_loss: 770128.3750\n",
      "Epoch 127/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 6252295.0000 - val_loss: 554912.3125\n",
      "Epoch 128/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 4063707.7500 - val_loss: 763944.8125\n",
      "Epoch 129/200\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 3150104.2500 - val_loss: 559294.1875\n",
      "Epoch 130/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 2694125.7500 - val_loss: 757906.8750\n",
      "Epoch 131/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 3484298.2500 - val_loss: 573117.6250\n",
      "Epoch 132/200\n",
      "78/78 [==============================] - 20s 252ms/step - loss: 4265149.5000 - val_loss: 672146.5625\n",
      "Epoch 133/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 4584554.5000 - val_loss: 621153.7500\n",
      "Epoch 134/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 4708126.0000 - val_loss: 628015.9375\n",
      "Epoch 135/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 5762722.5000 - val_loss: 1445840.0000\n",
      "Epoch 136/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 7489513.0000 - val_loss: 955060.5000\n",
      "Epoch 137/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 10804349.0000 - val_loss: 692193.8125\n",
      "Epoch 138/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 14485130.0000 - val_loss: 1409874.0000\n",
      "Epoch 139/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 17488932.0000 - val_loss: 767693.5000\n",
      "Epoch 140/200\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 17650028.0000 - val_loss: 828727.5625\n",
      "Epoch 141/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 14201464.0000 - val_loss: 447062.1562\n",
      "Epoch 142/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 7866426.5000 - val_loss: 813522.3125\n",
      "Epoch 143/200\n",
      "78/78 [==============================] - 20s 252ms/step - loss: 5453173.0000 - val_loss: 450938.5000\n",
      "Epoch 144/200\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 1202788.0000 - val_loss: 611584.9375\n",
      "Epoch 145/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 1328874.7500 - val_loss: 337433.2500\n",
      "Epoch 146/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 1727253.7500 - val_loss: 2392168.2500\n",
      "Epoch 147/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 560131.6250 - val_loss: 804362.5000\n",
      "Epoch 148/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 2935596.5000 - val_loss: 528370.2500\n",
      "Epoch 149/200\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 8605361.0000 - val_loss: 720610.4375\n",
      "Epoch 150/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 6356192.0000 - val_loss: 467042.4062\n",
      "Epoch 151/200\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 5283547.5000 - val_loss: 1380996.6250\n",
      "Epoch 152/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 4236596.5000 - val_loss: 1168805.2500\n",
      "Epoch 153/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 3313107.0000 - val_loss: 1154966.3750\n",
      "Epoch 154/200\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 4466919.0000 - val_loss: 611091.1250\n",
      "Epoch 155/200\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 2905729.2500 - val_loss: 383574.4688\n",
      "Epoch 156/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 2709685.5000 - val_loss: 321978.4688\n",
      "Epoch 157/200\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 2631080.5000 - val_loss: 328154.5312\n",
      "Epoch 158/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 2546316.7500 - val_loss: 321213.5625\n",
      "Epoch 159/200\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 2639672.0000 - val_loss: 450661.7500\n",
      "Epoch 160/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 2658425.5000 - val_loss: 359916.2500\n",
      "Epoch 161/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 2783923.2500 - val_loss: 512642.5000\n",
      "Epoch 162/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 3617310.5000 - val_loss: 351508.1250\n",
      "Epoch 163/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 4283781.5000 - val_loss: 518363.6562\n",
      "Epoch 164/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 4301559.0000 - val_loss: 234164.3594\n",
      "Epoch 165/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 4277198.5000 - val_loss: 475942.4688\n",
      "Epoch 166/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 4074855.2500 - val_loss: 945562.1250\n",
      "Epoch 167/200\n",
      "78/78 [==============================] - 21s 270ms/step - loss: 3291132.5000 - val_loss: 11582357.0000\n",
      "Epoch 168/200\n",
      "78/78 [==============================] - 21s 264ms/step - loss: 2905506.5000 - val_loss: 1508652.6250\n",
      "Epoch 169/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 2036023.2500 - val_loss: 598464.1875\n",
      "Epoch 170/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 1669415.0000 - val_loss: 1296274.5000\n",
      "Epoch 171/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 1832509.6250 - val_loss: 700699.6250\n",
      "Epoch 172/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 1104865.8750 - val_loss: 965546.7500\n",
      "Epoch 173/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 824160.5625 - val_loss: 691962.4375\n",
      "Epoch 174/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 401179.9688 - val_loss: 1248472.0000\n",
      "Epoch 175/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 260004.0469 - val_loss: 541546.5000\n",
      "Epoch 176/200\n",
      "78/78 [==============================] - 21s 267ms/step - loss: 307209.6875 - val_loss: 667294.5000\n",
      "Epoch 177/200\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 663274.0625 - val_loss: 314838.5000\n",
      "Epoch 178/200\n",
      "78/78 [==============================] - 21s 270ms/step - loss: 903863.7500 - val_loss: 297458.2812\n",
      "Epoch 179/200\n",
      "78/78 [==============================] - 21s 275ms/step - loss: 889321.0000 - val_loss: 216508.0469\n",
      "Epoch 180/200\n",
      "78/78 [==============================] - 21s 266ms/step - loss: 1517403.0000 - val_loss: 291558.6875\n",
      "Epoch 181/200\n",
      "78/78 [==============================] - 21s 273ms/step - loss: 2748114.7500 - val_loss: 164559.5938\n",
      "Epoch 182/200\n",
      "78/78 [==============================] - 23s 289ms/step - loss: 4648784.5000 - val_loss: 174241.0156\n",
      "Epoch 183/200\n",
      "78/78 [==============================] - 23s 293ms/step - loss: 5598351.5000 - val_loss: 262707.9375\n",
      "Epoch 184/200\n",
      "78/78 [==============================] - 22s 283ms/step - loss: 3598821.0000 - val_loss: 151861.3594\n",
      "Epoch 185/200\n",
      "78/78 [==============================] - 22s 285ms/step - loss: 20520158.0000 - val_loss: 1188817.3750\n",
      "Epoch 186/200\n",
      "78/78 [==============================] - 25s 318ms/step - loss: 133656.9844 - val_loss: 206984.3906\n",
      "Epoch 187/200\n",
      "78/78 [==============================] - 25s 315ms/step - loss: 156749.5625 - val_loss: 147589.6875\n",
      "Epoch 188/200\n",
      "78/78 [==============================] - 23s 296ms/step - loss: 388301.9062 - val_loss: 151813.7500\n",
      "Epoch 189/200\n",
      "78/78 [==============================] - 21s 269ms/step - loss: 2480923.2500 - val_loss: 148787.6406\n",
      "Epoch 190/200\n",
      "78/78 [==============================] - 21s 264ms/step - loss: 621911.5625 - val_loss: 217049.0312\n",
      "Epoch 191/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 415332.3438 - val_loss: 108275.4219\n",
      "Epoch 192/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 746120.6250 - val_loss: 89210.0547\n",
      "Epoch 193/200\n",
      "78/78 [==============================] - 21s 264ms/step - loss: 298628.2188 - val_loss: 106511.0625\n",
      "Epoch 194/200\n",
      "78/78 [==============================] - 21s 271ms/step - loss: 163864.8438 - val_loss: 122948.1250\n",
      "Epoch 195/200\n",
      "78/78 [==============================] - 23s 295ms/step - loss: 228327.2344 - val_loss: 98893.3984\n",
      "Epoch 196/200\n",
      "78/78 [==============================] - 21s 263ms/step - loss: 180731.9688 - val_loss: 176094.2500\n",
      "Epoch 197/200\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 1991825.5000 - val_loss: 83227.5312\n",
      "Epoch 198/200\n",
      "78/78 [==============================] - 23s 295ms/step - loss: 71829.8750 - val_loss: 74122.5234\n",
      "Epoch 199/200\n",
      "78/78 [==============================] - 22s 275ms/step - loss: 546308.7500 - val_loss: 81729.0547\n",
      "Epoch 200/200\n",
      "78/78 [==============================] - 21s 276ms/step - loss: 241132.3750 - val_loss: 79822.1875\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [38]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# fit model\u001B[39;00m\n\u001B[0;32m     17\u001B[0m trained \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(x_train, y_train, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, validation_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m \u001B[43mplot_model_history\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrained\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [33]\u001B[0m, in \u001B[0;36mplot_model_history\u001B[1;34m(history)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_model_history\u001B[39m(history):\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# summarize history for accuracy\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m     plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     26\u001B[0m     plt\u001B[38;5;241m.\u001B[39mplot(history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     27\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "# model layers\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "print(np.any(~np.isfinite(x_train)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Masking(mask_value=0.,\n",
    "                                  input_shape=(timesteps, n_features)))\n",
    "model.add(LSTM(50, activation='relu', input_shape=(timesteps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "trained = model.fit(x_train, y_train, epochs=200, validation_split=0.2)\n",
    "model.save(\"./models/simple\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "78/78 [==============================] - 23s 260ms/step - loss: 3382419652608.0000 - val_loss: 19818905600.0000\n",
      "Epoch 2/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 97052377088.0000 - val_loss: 196645344.0000\n",
      "Epoch 3/200\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 4428836864.0000 - val_loss: 33291460.0000\n",
      "Epoch 4/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 2220482816.0000 - val_loss: 22289818.0000\n",
      "Epoch 5/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 18489628672.0000 - val_loss: 72189472.0000\n",
      "Epoch 6/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 3448571648.0000 - val_loss: 12536550.0000\n",
      "Epoch 7/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 7987371520.0000 - val_loss: 12430032.0000\n",
      "Epoch 8/200\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 6507437568.0000 - val_loss: 381299840.0000\n",
      "Epoch 9/200\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 10240647168.0000 - val_loss: 22673502.0000\n",
      "Epoch 10/200\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 5337845248.0000 - val_loss: 11501036.0000\n",
      "Epoch 11/200\n",
      "78/78 [==============================] - 22s 279ms/step - loss: 2380124672.0000 - val_loss: 222097776.0000\n",
      "Epoch 12/200\n",
      "78/78 [==============================] - 26s 334ms/step - loss: 1565841664.0000 - val_loss: 11686342.0000\n",
      "Epoch 13/200\n",
      "78/78 [==============================] - 25s 314ms/step - loss: 1941686528.0000 - val_loss: 14940167.0000\n",
      "Epoch 14/200\n",
      "78/78 [==============================] - 22s 285ms/step - loss: 1390721152.0000 - val_loss: 9113846.0000\n",
      "Epoch 15/200\n",
      "78/78 [==============================] - 22s 279ms/step - loss: 441629824.0000 - val_loss: 2810004480.0000\n",
      "Epoch 16/200\n",
      "78/78 [==============================] - 22s 278ms/step - loss: 777619840.0000 - val_loss: 16574966.0000\n",
      "Epoch 17/200\n",
      "78/78 [==============================] - 22s 283ms/step - loss: 485390304.0000 - val_loss: 5521801.0000\n",
      "Epoch 18/200\n",
      "78/78 [==============================] - 21s 268ms/step - loss: 1381927040.0000 - val_loss: 24120220.0000\n",
      "Epoch 19/200\n",
      "78/78 [==============================] - 29s 380ms/step - loss: 100427632.0000 - val_loss: 841232064.0000\n",
      "Epoch 20/200\n",
      "78/78 [==============================] - 33s 426ms/step - loss: 2537126912.0000 - val_loss: 10252117.0000\n",
      "Epoch 21/200\n",
      "64/78 [=======================>......] - ETA: 17s - loss: 871489280.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmse\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# fit model\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m trained \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./models/simple\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\engine\\training.py:1414\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1412\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs  \u001B[38;5;66;03m# No error, now safe to assign to logs.\u001B[39;00m\n\u001B[0;32m   1413\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[1;32m-> 1414\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[0;32m   1416\u001B[0m   \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\callbacks.py:438\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[0;32m    432\u001B[0m \n\u001B[0;32m    433\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[0;32m    436\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[1;32m--> 438\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\callbacks.py:297\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    295\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 297\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    299\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    300\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Expected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\callbacks.py:318\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    315\u001B[0m   batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[0;32m    316\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[1;32m--> 318\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[0;32m    321\u001B[0m   end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\callbacks.py:356\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m    355\u001B[0m   hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[1;32m--> 356\u001B[0m   \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[0;32m    359\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\callbacks.py:1034\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m-> 1034\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\callbacks.py:1106\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1102\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m add_seen\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1105\u001B[0m   \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[1;32m-> 1106\u001B[0m   logs \u001B[38;5;241m=\u001B[39m \u001B[43mtf_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync_to_numpy_or_python_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1107\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen, \u001B[38;5;28mlist\u001B[39m(logs\u001B[38;5;241m.\u001B[39mitems()), finalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n\u001B[0;32m    605\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(t) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m--> 607\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_single_numpy_or_python_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001B[0m, in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    912\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    913\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    915\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 916\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    917\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    912\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    913\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    915\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 916\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    917\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[0;32m    599\u001B[0m   \u001B[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[0;32m    600\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, tf\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 601\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    602\u001B[0m   \u001B[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001B[39;00m\n\u001B[0;32m    603\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mgeneric)):\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1136\u001B[0m \u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[0;32m   1137\u001B[0m \n\u001B[0;32m   1138\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1156\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[0;32m   1157\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1158\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[1;32m-> 1159\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1124\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1126\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Masking(mask_value=0.,\n",
    "                                  input_shape=(timesteps, n_features)))\n",
    "model.add(LSTM(50, activation='relu', input_shape=(timesteps, n_features)))\n",
    "model.add(Dense(1))\n",
    "optimizer = optimizers.Adam(clipvalue=1.0)\n",
    "optimizer.learning_rate = 0.0001\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# fit model\n",
    "trained = model.fit(x_train, y_train, epochs=200, validation_split=0.2, batch_size=32)\n",
    "model.save(\"./models/simple\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -10.137939 ]\n",
      " [ -21.527594 ]\n",
      " [-165.05948  ]\n",
      " ...\n",
      " [   2.94411  ]\n",
      " [   1.7421514]\n",
      " [  20.960835 ]]\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_train, verbose=0)\n",
    "print(yhat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stacked LSTM\n",
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      " 1/78 [..............................] - ETA: 5:26 - loss: 236655624192.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m stacked_model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmse\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# fit model\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mstacked_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\keras\\engine\\training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2451\u001B[0m   (graph_function,\n\u001B[0;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1863\u001B[0m     args,\n\u001B[0;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1865\u001B[0m     executing_eagerly)\n\u001B[0;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\code\\TESSEpochTime\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# model layers\n",
    "stacked_model = Sequential()\n",
    "stacked_model.add(tf.keras.layers.Masking(mask_value=0.,\n",
    "                                  input_shape=(timesteps, n_features)))\n",
    "stacked_model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(timesteps, n_features)))\n",
    "stacked_model.add(LSTM(50, activation='relu'))\n",
    "stacked_model.add(Dense(1))\n",
    "stacked_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "history = stacked_model.fit(x_train, y_train, epochs=200, batch_size=32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.66204844e+05]\n",
      " [ 2.21391816e+04]\n",
      " [ 1.02095610e+07]\n",
      " [-3.29108359e+04]\n",
      " [ 6.62721387e+03]\n",
      " [ 7.28425879e+03]\n",
      " [ 7.66529724e+02]\n",
      " [ 1.44949541e+04]\n",
      " [ 2.29155820e+04]\n",
      " [-2.04046699e+04]\n",
      " [ 1.95377356e+03]\n",
      " [ 8.93780391e+04]\n",
      " [ 1.80358945e+04]\n",
      " [ 4.03771533e+03]\n",
      " [ 4.50096016e+04]\n",
      " [ 3.32476875e+04]\n",
      " [ 9.87614648e+03]\n",
      " [ 1.17269617e+03]\n",
      " [ 6.76540703e+04]\n",
      " [-1.77878510e+02]\n",
      " [-1.67209778e+02]\n",
      " [ 1.71430359e+01]\n",
      " [-7.02323792e+02]\n",
      " [ 6.73518828e+04]\n",
      " [-9.66421777e+03]\n",
      " [ 4.09060195e+04]\n",
      " [-9.82723389e+01]\n",
      " [ 1.76611328e+02]\n",
      " [ 8.16583047e+04]\n",
      " [ 2.07215519e+01]\n",
      " [ 6.52561914e+03]\n",
      " [ 3.09978442e+03]\n",
      " [ 1.69183044e+02]\n",
      " [ 1.91399216e+02]\n",
      " [ 3.14218625e+05]\n",
      " [ 4.94676285e+01]\n",
      " [ 1.05616766e+05]\n",
      " [ 4.50785531e+05]\n",
      " [ 4.15685107e+03]\n",
      " [ 2.11975952e+03]\n",
      " [ 1.84379922e+04]\n",
      " [ 1.26441589e+02]\n",
      " [ 7.14608398e+03]\n",
      " [ 2.62070984e+02]\n",
      " [ 5.81176611e+03]\n",
      " [-3.82126801e+02]\n",
      " [ 7.98217224e+02]\n",
      " [ 3.62462695e+04]\n",
      " [ 2.08336688e+05]\n",
      " [-2.45978928e+02]\n",
      " [-7.01234283e+01]\n",
      " [-1.47476501e+03]\n",
      " [ 1.09473164e+05]\n",
      " [ 6.13968086e+04]\n",
      " [ 8.16580688e+05]\n",
      " [ 3.72309961e+04]\n",
      " [ 6.02768616e+02]\n",
      " [ 1.00319609e+04]\n",
      " [ 4.09047031e+05]\n",
      " [ 2.83266735e+01]\n",
      " [ 9.14164297e+04]\n",
      " [-5.28965225e+01]\n",
      " [ 8.37919375e+04]\n",
      " [ 1.02896350e+03]\n",
      " [-7.38034515e+01]\n",
      " [ 1.31171387e+04]\n",
      " [ 1.29315086e+05]\n",
      " [ 4.09386562e+05]\n",
      " [ 1.35130000e+05]\n",
      " [ 8.04165234e+04]\n",
      " [ 4.53063263e+02]\n",
      " [ 2.93123184e+04]\n",
      " [ 9.43928418e+03]\n",
      " [ 6.94007422e+04]\n",
      " [ 2.96265289e+02]\n",
      " [ 2.41431309e+04]\n",
      " [-3.51499023e+02]\n",
      " [ 1.85484688e+05]\n",
      " [ 7.12384094e+02]\n",
      " [ 5.58310062e+05]\n",
      " [ 6.29995544e+02]\n",
      " [ 4.39293789e+04]\n",
      " [ 1.83803633e+04]\n",
      " [ 2.60176270e+02]\n",
      " [ 3.14992312e+05]\n",
      " [ 4.25752734e+04]\n",
      " [ 3.98450075e+06]\n",
      " [ 1.97156270e+04]\n",
      " [ 1.28724790e+07]\n",
      " [-4.79138702e+02]\n",
      " [ 1.97088281e+05]\n",
      " [ 1.60085239e+01]\n",
      " [ 1.24211846e+04]\n",
      " [-1.07772906e+05]\n",
      " [ 8.73483312e+05]\n",
      " [-2.99654358e+02]\n",
      " [ 6.99041000e+05]\n",
      " [ 1.40698562e+05]\n",
      " [ 5.00819336e+03]\n",
      " [ 1.03396156e+05]\n",
      " [ 1.63505828e+05]\n",
      " [ 5.72606750e+05]\n",
      " [ 9.86983516e+04]\n",
      " [-3.15327661e+03]\n",
      " [ 1.18061201e+04]\n",
      " [-7.37089941e+03]\n",
      " [ 8.96156836e+03]\n",
      " [ 8.02587830e+02]\n",
      " [ 7.97266094e+04]\n",
      " [ 2.49862656e+05]\n",
      " [-1.01908150e+02]\n",
      " [ 1.25824188e+05]\n",
      " [ 2.52569312e+05]\n",
      " [ 2.05354111e+02]\n",
      " [-6.34531006e+02]\n",
      " [ 3.26619250e+05]\n",
      " [ 1.36917322e+03]\n",
      " [ 4.86978662e+03]\n",
      " [-8.11294373e+02]\n",
      " [ 5.50705029e+03]\n",
      " [-4.59134796e+02]\n",
      " [ 5.04867096e+01]\n",
      " [ 2.24669453e+05]\n",
      " [ 1.47956909e+03]\n",
      " [ 1.45533862e+03]\n",
      " [ 4.38455969e+05]\n",
      " [ 3.07848062e+05]\n",
      " [ 4.35376914e+04]\n",
      " [ 2.74695312e+05]\n",
      " [ 4.35937439e+02]\n",
      " [ 7.95135234e+04]\n",
      " [ 1.31834094e+05]\n",
      " [-1.80781921e+03]\n",
      " [ 1.86220288e+03]\n",
      " [-2.03818848e+02]\n",
      " [ 2.10231470e+03]\n",
      " [ 3.40004062e+05]\n",
      " [ 1.46184937e+03]\n",
      " [ 8.62799149e+01]\n",
      " [ 6.66317285e+03]\n",
      " [ 6.09074646e+02]\n",
      " [-8.86784180e+03]\n",
      " [-1.60315598e+02]\n",
      " [ 5.33402617e+04]\n",
      " [-7.38670715e+02]\n",
      " [ 6.78276062e+02]\n",
      " [-8.17424774e+01]\n",
      " [ 7.90075312e+05]\n",
      " [-3.65674219e+04]\n",
      " [ 2.89739355e+03]\n",
      " [-3.48416168e+02]\n",
      " [ 3.67898346e+02]\n",
      " [ 2.29828262e+04]\n",
      " [ 1.49921250e+04]\n",
      " [-1.60981979e+02]\n",
      " [ 3.03040161e+02]\n",
      " [ 1.18393656e+05]\n",
      " [ 6.71881714e+02]\n",
      " [ 2.58043098e+00]\n",
      " [ 1.87864023e+04]\n",
      " [ 6.10665833e+02]\n",
      " [-2.38596191e+02]\n",
      " [-1.13880762e+03]\n",
      " [-5.75398682e+02]\n",
      " [-2.94830139e+02]\n",
      " [ 1.81213638e+02]\n",
      " [ 1.73310566e+04]\n",
      " [ 2.62327461e+04]\n",
      " [ 1.01545469e+04]\n",
      " [-5.93446655e+01]\n",
      " [ 1.63946309e+04]\n",
      " [-3.39270935e+02]\n",
      " [-2.59978363e+02]\n",
      " [-2.96551514e+01]\n",
      " [-4.47050476e+02]\n",
      " [ 1.43971252e+03]\n",
      " [-3.84609131e+02]\n",
      " [-2.05097870e+02]\n",
      " [-1.92404678e+02]\n",
      " [ 7.24333105e+03]\n",
      " [ 4.93987366e+02]\n",
      " [ 2.78882690e+02]\n",
      " [ 1.64744983e+03]\n",
      " [ 4.74126434e+02]\n",
      " [ 3.38944214e+02]\n",
      " [ 1.72923303e+03]\n",
      " [ 1.00006621e+04]\n",
      " [ 1.13220109e+05]\n",
      " [ 1.28838531e+05]\n",
      " [ 7.39710327e+02]\n",
      " [ 1.65142175e+03]\n",
      " [ 4.06222383e+04]\n",
      " [ 9.40676594e+00]\n",
      " [ 1.13876258e+05]\n",
      " [ 4.07931438e+05]\n",
      " [-3.24647095e+03]\n",
      " [ 4.37480898e+04]\n",
      " [-9.05860291e+02]\n",
      " [-3.84757599e+02]\n",
      " [ 1.97136973e+04]\n",
      " [ 3.44476050e+03]\n",
      " [ 2.86476094e+04]\n",
      " [ 2.19203688e+05]\n",
      " [ 6.32278125e+05]\n",
      " [ 1.55076422e+05]\n",
      " [ 2.05293770e+04]\n",
      " [ 8.08824688e+04]\n",
      " [ 2.20109448e+03]\n",
      " [ 1.59054250e+05]\n",
      " [ 2.81049629e+04]\n",
      " [-9.93316650e+02]\n",
      " [ 1.94044531e+04]\n",
      " [ 2.72056177e+03]\n",
      " [ 2.93256250e+05]\n",
      " [-1.28560264e+04]\n",
      " [-3.64497261e+01]\n",
      " [-3.16745337e+03]\n",
      " [ 8.39447188e+04]\n",
      " [ 3.49116577e+03]\n",
      " [-7.36244812e+01]\n",
      " [ 6.05920273e+04]\n",
      " [ 5.48205781e+04]\n",
      " [-3.16821423e+01]\n",
      " [ 1.27804422e+05]\n",
      " [ 5.69902031e+04]\n",
      " [ 3.85247344e+04]\n",
      " [ 1.64556538e+06]\n",
      " [ 1.47317444e+02]\n",
      " [-1.17932915e+02]\n",
      " [-2.14311466e+01]\n",
      " [ 8.43854431e+02]\n",
      " [ 2.83976914e+04]\n",
      " [ 2.16674375e+05]\n",
      " [ 2.08973120e+03]\n",
      " [ 1.47303562e+05]\n",
      " [ 3.06958838e+03]\n",
      " [-6.88316956e+01]\n",
      " [ 7.03373438e+04]\n",
      " [ 9.89856140e+02]\n",
      " [ 3.51167656e+04]\n",
      " [ 1.18562451e+03]\n",
      " [-8.31638977e+02]\n",
      " [ 3.88429766e+04]\n",
      " [ 6.60043672e+04]\n",
      " [ 2.37328938e+05]\n",
      " [ 1.66350208e+03]\n",
      " [-3.42541046e+02]\n",
      " [ 3.54526094e+04]\n",
      " [ 3.47239844e+03]\n",
      " [ 1.81931816e+04]\n",
      " [-1.13710889e+03]\n",
      " [ 5.99579224e+02]\n",
      " [ 3.67918365e+02]\n",
      " [ 2.36578770e+04]\n",
      " [ 7.84540771e+03]\n",
      " [ 3.27256909e+03]\n",
      " [ 2.27157031e+03]\n",
      " [ 2.32401218e+01]\n",
      " [-3.29519562e+02]\n",
      " [ 8.47507324e+01]\n",
      " [ 1.20364570e+05]\n",
      " [ 2.63883594e+04]\n",
      " [ 3.76263824e+02]\n",
      " [-4.13373486e+03]\n",
      " [ 1.68817505e+02]\n",
      " [ 2.01607239e+03]\n",
      " [ 1.64406719e+04]\n",
      " [-2.51489594e+02]\n",
      " [ 7.62713196e+02]\n",
      " [-3.15110840e+02]\n",
      " [ 4.89499102e+04]\n",
      " [ 1.76471445e+04]\n",
      " [ 4.50368115e+03]\n",
      " [ 1.05785510e+03]\n",
      " [ 2.49044875e+05]\n",
      " [ 1.38852258e+03]\n",
      " [-2.50495020e+03]\n",
      " [-1.10258758e-01]\n",
      " [ 6.00851746e+01]\n",
      " [ 2.77971641e+04]\n",
      " [-8.07580261e+02]\n",
      " [ 9.56209869e+01]\n",
      " [-3.87207428e+02]\n",
      " [ 1.95216350e+06]\n",
      " [ 8.52303594e+04]\n",
      " [-2.67136660e+04]\n",
      " [ 7.34518066e+02]\n",
      " [-6.04211243e+02]\n",
      " [ 1.14342697e+02]\n",
      " [ 6.30359473e+03]\n",
      " [ 5.14135039e+04]\n",
      " [-6.46376587e+02]\n",
      " [-6.68459244e+01]\n",
      " [ 3.41868011e+02]\n",
      " [-9.62453320e+03]\n",
      " [ 1.29786982e+04]\n",
      " [ 5.42966797e+04]\n",
      " [-2.23015060e+02]\n",
      " [-6.85550415e+02]\n",
      " [ 8.15007520e+03]\n",
      " [-2.32649628e+02]\n",
      " [ 3.97821564e+02]\n",
      " [ 1.30360664e+04]\n",
      " [ 1.57269082e+04]\n",
      " [ 1.46418372e+03]\n",
      " [-5.35049515e+01]\n",
      " [ 2.08642383e+04]\n",
      " [ 6.97114746e+02]\n",
      " [ 1.32281738e+03]\n",
      " [ 1.35466748e+03]\n",
      " [ 7.62476270e+03]\n",
      " [ 6.12570459e+03]\n",
      " [-5.58458057e+03]\n",
      " [ 7.50346484e+03]\n",
      " [-3.59314178e+02]\n",
      " [-3.15378754e+02]\n",
      " [ 5.72015686e+02]\n",
      " [ 7.05636414e+02]\n",
      " [-1.00623706e+03]\n",
      " [-1.42702576e+02]\n",
      " [ 2.72299238e+04]\n",
      " [ 1.37191406e+04]\n",
      " [-1.48372705e+03]\n",
      " [ 1.14415869e+03]\n",
      " [ 6.21624939e+02]\n",
      " [ 4.83195068e+03]\n",
      " [-6.86970398e+02]\n",
      " [ 3.47262915e+03]\n",
      " [-4.20561584e+02]\n",
      " [ 4.38249170e+03]\n",
      " [ 2.68662754e+04]\n",
      " [ 5.75664001e+02]\n",
      " [ 3.42486992e+01]\n",
      " [ 1.50641760e+03]\n",
      " [-2.86657776e+02]\n",
      " [ 4.25326477e+02]\n",
      " [ 8.25907166e+02]\n",
      " [-3.45931616e+03]\n",
      " [ 4.71330948e+01]\n",
      " [ 3.44099457e+02]\n",
      " [ 2.35335266e+05]\n",
      " [ 2.06852310e+02]\n",
      " [-3.89487701e+02]\n",
      " [ 1.36302578e+05]\n",
      " [ 2.05737520e+04]\n",
      " [-3.42143286e+03]\n",
      " [-5.12159790e+02]\n",
      " [ 5.50553809e+03]\n",
      " [ 1.25988828e+04]\n",
      " [-1.97767334e+03]\n",
      " [-1.47529333e+03]\n",
      " [ 2.43794316e+04]\n",
      " [ 3.77846094e+04]\n",
      " [ 1.03172773e+05]\n",
      " [ 9.74244843e+01]\n",
      " [ 8.00986084e+02]\n",
      " [-1.14199249e+02]\n",
      " [ 1.11406961e+05]\n",
      " [ 6.51406133e+04]\n",
      " [ 1.62201006e+04]\n",
      " [-2.28839624e+03]\n",
      " [-3.33575928e+03]\n",
      " [-3.01551727e+02]\n",
      " [-7.78232031e+04]\n",
      " [-6.74309753e+02]\n",
      " [ 7.67733398e+03]\n",
      " [-1.64151897e+01]\n",
      " [-2.72401294e+03]\n",
      " [-4.70149121e+03]\n",
      " [-5.75270625e+04]\n",
      " [ 4.76691016e+03]\n",
      " [ 1.91762500e+04]\n",
      " [ 2.10916840e+02]\n",
      " [ 5.02103394e+02]\n",
      " [-1.17302353e+02]\n",
      " [ 4.32708496e+03]\n",
      " [ 3.07111938e+03]\n",
      " [ 1.63473578e+05]\n",
      " [ 2.76005981e+02]\n",
      " [-2.26368317e+02]\n",
      " [-2.99662262e+02]\n",
      " [ 9.06034766e+03]\n",
      " [ 3.96246558e+03]\n",
      " [ 1.28276182e+04]\n",
      " [ 1.19034277e+04]\n",
      " [ 7.13806689e+03]\n",
      " [ 5.96783386e+02]\n",
      " [ 7.88496484e+03]\n",
      " [ 3.22856421e+03]\n",
      " [ 2.53959869e+02]\n",
      " [-1.31533923e+03]\n",
      " [ 1.93295361e+03]\n",
      " [ 7.11543506e+03]\n",
      " [ 6.39069629e+03]\n",
      " [-1.97772552e+02]\n",
      " [ 1.37404407e+03]\n",
      " [-6.23526211e+04]\n",
      " [ 4.45207812e+04]\n",
      " [ 2.39208145e+04]\n",
      " [-4.17406494e+03]\n",
      " [ 3.04072021e+02]\n",
      " [-7.97336094e+04]\n",
      " [ 3.46536987e+03]\n",
      " [-2.63848053e+02]\n",
      " [ 3.67544952e+02]\n",
      " [ 3.45485474e+03]\n",
      " [ 5.81064514e+02]\n",
      " [ 1.61894440e+02]\n",
      " [-2.39488516e+04]\n",
      " [ 1.25950820e+04]\n",
      " [ 2.16428101e+03]\n",
      " [-2.60198389e+03]\n",
      " [ 3.87548584e+03]\n",
      " [ 1.27731506e+03]\n",
      " [-5.46060852e+02]\n",
      " [-2.34672729e+02]\n",
      " [ 1.15106213e+03]\n",
      " [ 8.29428024e+01]\n",
      " [ 1.01206520e+02]\n",
      " [ 2.29389502e+03]\n",
      " [ 1.51891626e+03]\n",
      " [ 3.71322852e+03]\n",
      " [ 1.26054724e+03]\n",
      " [-2.54525659e+03]\n",
      " [ 1.23834367e+05]\n",
      " [ 1.35237150e+07]\n",
      " [ 5.86486328e+03]\n",
      " [ 3.29667212e+03]\n",
      " [ 3.88603540e+03]\n",
      " [-1.79604626e+03]\n",
      " [ 5.81824000e+05]\n",
      " [ 5.85383398e+03]\n",
      " [ 4.64746613e+02]\n",
      " [ 6.54127380e+02]\n",
      " [ 3.24545410e+02]\n",
      " [-1.75001294e+03]\n",
      " [ 1.27291924e+04]\n",
      " [-5.38058281e+00]\n",
      " [ 8.00446797e+04]\n",
      " [ 1.04633945e+05]\n",
      " [ 1.05058369e+04]\n",
      " [ 2.39554108e+02]\n",
      " [ 6.55253242e+04]\n",
      " [ 4.65650293e+03]\n",
      " [ 4.49780029e+03]\n",
      " [ 8.04468018e+02]\n",
      " [-1.01359520e+02]\n",
      " [ 8.36693115e+02]\n",
      " [-4.46741211e+03]\n",
      " [-1.84095871e+02]\n",
      " [ 3.15441498e+02]\n",
      " [-2.51141800e+02]\n",
      " [-1.31136797e+04]\n",
      " [ 2.46477375e+01]\n",
      " [ 3.25207031e+04]\n",
      " [ 4.58429626e+02]\n",
      " [-1.32918982e+03]\n",
      " [ 1.26700366e+03]\n",
      " [ 5.04207227e+04]\n",
      " [-1.15653027e+03]\n",
      " [-3.20549438e+03]\n",
      " [ 9.21737488e+02]\n",
      " [ 7.30239355e+03]\n",
      " [ 1.95269012e+02]\n",
      " [ 8.88120056e+02]\n",
      " [ 7.41603455e+02]\n",
      " [ 2.51381317e+02]\n",
      " [-1.35557312e+03]\n",
      " [-9.49652252e+01]\n",
      " [ 6.94197083e+02]\n",
      " [-1.05596494e+04]\n",
      " [ 6.83123047e+02]\n",
      " [ 1.12086201e+04]\n",
      " [ 1.57294116e+03]\n",
      " [-4.68987207e+03]\n",
      " [ 1.02216077e+03]\n",
      " [ 6.79869385e+02]\n",
      " [ 2.03378296e+02]\n",
      " [ 5.90916809e+02]\n",
      " [ 1.96447812e+04]\n",
      " [ 1.02791481e+02]\n",
      " [ 3.52823792e+02]\n",
      " [ 6.15973877e+02]\n",
      " [ 2.73088428e+03]\n",
      " [ 5.73218848e+03]\n",
      " [ 3.05600781e+03]\n",
      " [ 9.44800195e+03]\n",
      " [ 3.88478594e+04]\n",
      " [ 2.99663916e+03]\n",
      " [-3.73385498e+02]\n",
      " [ 2.06645874e+03]\n",
      " [ 5.07179500e+06]\n",
      " [ 5.53938062e+05]\n",
      " [-7.44115186e+03]\n",
      " [ 1.67806504e+04]\n",
      " [ 2.36177383e+04]\n",
      " [ 4.21943066e+03]\n",
      " [ 7.51580029e+03]\n",
      " [ 8.68351484e+04]\n",
      " [ 3.80827148e+02]\n",
      " [ 1.55746234e+05]\n",
      " [-1.57065833e+03]\n",
      " [ 1.44331328e+04]\n",
      " [ 1.92041660e+04]\n",
      " [-1.91762573e+02]\n",
      " [ 3.08235962e+02]\n",
      " [-6.91866211e+02]\n",
      " [ 9.67692578e+03]\n",
      " [ 4.65831738e+03]\n",
      " [ 2.32592627e+03]\n",
      " [ 3.84148315e+02]\n",
      " [-2.83556152e+04]\n",
      " [ 1.02968266e+05]\n",
      " [ 1.14545156e+05]\n",
      " [ 2.10134277e+04]\n",
      " [ 5.11891205e+02]\n",
      " [ 7.87515430e+03]\n",
      " [-5.71620728e+02]\n",
      " [ 3.59128633e+04]\n",
      " [ 3.64292500e+05]\n",
      " [ 3.45852500e+04]\n",
      " [ 1.07360914e+05]\n",
      " [-2.01464209e+03]\n",
      " [ 5.58641968e+02]\n",
      " [ 3.38886641e+04]\n",
      " [ 3.85441899e+00]\n",
      " [ 1.49478184e+04]\n",
      " [ 7.34240646e+01]\n",
      " [ 1.09736844e+05]\n",
      " [ 6.00487122e+02]\n",
      " [ 1.64840561e+02]\n",
      " [ 2.81543281e+04]\n",
      " [ 3.17580645e+04]\n",
      " [ 1.56035576e+04]\n",
      " [-5.71277466e+02]\n",
      " [ 3.87337250e+02]\n",
      " [ 4.93659844e+04]\n",
      " [ 2.13580127e+03]\n",
      " [ 6.11230039e+04]\n",
      " [ 4.43500049e+03]\n",
      " [ 5.32597900e+02]\n",
      " [ 2.09158521e+03]\n",
      " [ 4.07526523e+04]\n",
      " [ 1.52856470e+03]\n",
      " [ 2.16380625e+04]\n",
      " [ 8.84162140e+01]\n",
      " [ 3.20137231e+03]\n",
      " [ 2.42196387e+04]\n",
      " [ 8.45699141e+04]\n",
      " [ 5.98466675e+02]\n",
      " [ 2.10576599e+02]\n",
      " [ 7.00254688e+04]\n",
      " [ 2.12706030e+03]\n",
      " [ 3.07530762e+02]\n",
      " [ 4.65323828e+04]\n",
      " [ 2.63286055e+04]\n",
      " [-3.46574066e+02]\n",
      " [ 2.94687949e+04]\n",
      " [-1.35953760e+03]\n",
      " [ 4.31092407e+02]\n",
      " [ 1.51168457e+02]\n",
      " [-3.48448601e+01]\n",
      " [ 3.03038269e+02]\n",
      " [ 1.01309613e+03]\n",
      " [ 6.33554883e+03]\n",
      " [ 6.39463684e+02]\n",
      " [ 1.03512665e+02]\n",
      " [ 7.53490781e+04]\n",
      " [ 1.05812109e+04]\n",
      " [ 2.30081345e+02]\n",
      " [ 1.60008975e+04]\n",
      " [ 8.94307734e+04]\n",
      " [ 3.79520378e+01]\n",
      " [ 9.92325287e+01]\n",
      " [ 3.80366914e+04]\n",
      " [ 1.94760156e+03]\n",
      " [ 1.56388672e+04]\n",
      " [ 2.92107086e+02]\n",
      " [-1.50288284e+02]\n",
      " [-2.65648779e+03]\n",
      " [ 3.05755762e+04]\n",
      " [ 4.50211312e+05]\n",
      " [ 1.19270811e+04]\n",
      " [ 8.84620667e+02]\n",
      " [-3.28733978e+02]\n",
      " [ 3.58906784e+01]\n",
      " [-1.24199133e+03]\n",
      " [ 8.13225000e+04]\n",
      " [-2.88860970e+01]\n",
      " [ 4.86016235e+02]\n",
      " [ 7.79296826e+03]\n",
      " [-3.45793976e+02]\n",
      " [ 3.75078760e+03]\n",
      " [ 2.37268696e+01]\n",
      " [ 9.29085449e+02]\n",
      " [-7.72801270e+02]\n",
      " [ 2.76314062e+05]\n",
      " [ 6.35500061e+02]\n",
      " [ 5.12846603e+01]\n",
      " [ 6.59578955e+03]\n",
      " [ 1.28726636e+03]\n",
      " [ 2.83538047e+04]\n",
      " [ 4.07576328e+04]\n",
      " [-3.54678945e+04]\n",
      " [ 1.98912338e+02]\n",
      " [ 8.75690674e+02]\n",
      " [-2.19668453e+05]\n",
      " [ 1.90491638e+01]\n",
      " [ 2.21056309e+01]\n",
      " [ 6.70098125e+04]\n",
      " [ 7.05602812e+04]\n",
      " [ 6.83042860e+00]\n",
      " [-3.98007080e+02]\n",
      " [ 4.52813965e+02]\n",
      " [ 1.36023087e+02]\n",
      " [-3.03807831e+02]\n",
      " [ 2.32058281e+04]\n",
      " [-1.49715543e+00]\n",
      " [ 1.75438248e+02]\n",
      " [-1.04267261e+03]\n",
      " [-6.03321350e+02]\n",
      " [ 4.47182178e+03]\n",
      " [ 3.55105469e+05]\n",
      " [ 5.63821758e+04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "dict_keys([])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = stacked_model.predict(x_test, verbose=0)\n",
    "print(yhat)\n",
    "stacked_model.history.history.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}